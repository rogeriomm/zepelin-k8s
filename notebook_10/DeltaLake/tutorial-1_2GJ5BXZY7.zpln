{
  "paragraphs": [
    {
      "text": "%md\n\n# Introduction\n\nThis is a tutorial for using spark [delta lake](https://delta.io/) in Zeppelin. You need to run the following paragraph first to load delta package.\n\n",
      "user": "anonymous",
      "dateUpdated": "2021-09-21 15:24:11.657",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch1\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eThis is a tutorial for using spark \u003ca href\u003d\"https://delta.io/\"\u003edelta lake\u003c/a\u003e in Zeppelin. You need to run the following paragraph first to load delta package.\u003c/p\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1632248651656_1228768679",
      "id": "paragraph_1588572279774_1507831415",
      "dateCreated": "2021-09-21 15:24:11.657",
      "status": "READY"
    },
    {
      "text": "%md\n```\n$SPARK_HOME/bin/spark-shell --verbose \\\n                            --packages io.delta:delta-core_2.12:1.0.0,org.apache.hadoop:hadoop-aws:$HADOOP_VERSION,org.apache.hadoop:hadoop-client:$HADOOP_VERSION,org.apache.hadoop:hadoop-common:$HADOOP_VERSION \\\n                            --conf \"spark.sql.extensions\u003dio.delta.sql.DeltaSparkSessionExtension\"  \\\n                            --conf \"spark.sql.catalog.spark_catalog\u003dorg.apache.spark.sql.delta.catalog.DeltaCatalog\"\n```\n",
      "user": "anonymous",
      "dateUpdated": "2021-10-08 09:27:17.765",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cpre\u003e\u003ccode\u003e$SPARK_HOME/bin/spark-shell --verbose \\\n                            --packages io.delta:delta-core_2.12:1.0.0,org.apache.hadoop:hadoop-aws:$HADOOP_VERSION,org.apache.hadoop:hadoop-client:$HADOOP_VERSION,org.apache.hadoop:hadoop-common:$HADOOP_VERSION \\\n                            --conf \u0026quot;spark.sql.extensions\u003dio.delta.sql.DeltaSparkSessionExtension\u0026quot;  \\\n                            --conf \u0026quot;spark.sql.catalog.spark_catalog\u003dorg.apache.spark.sql.delta.catalog.DeltaCatalog\u0026quot;\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1632248651657_66881631",
      "id": "paragraph_1631301936224_1666568166",
      "dateCreated": "2021-09-21 15:24:11.658",
      "dateStarted": "2021-10-08 09:27:17.765",
      "dateFinished": "2021-10-08 09:27:17.786",
      "status": "FINISHED"
    },
    {
      "text": "%md\n   * https://jboothomas.medium.com/running-spark-on-kubernetes-with-persistent-storage-24b7903bb40a: Running spark on Kubernetes with persistent storage",
      "user": "anonymous",
      "dateUpdated": "2021-10-08 17:04:59.884",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href\u003d\"https://jboothomas.medium.com/running-spark-on-kubernetes-with-persistent-storage-24b7903bb40a\"\u003ehttps://jboothomas.medium.com/running-spark-on-kubernetes-with-persistent-storage-24b7903bb40a\u003c/a\u003e: Running spark on Kubernetes with persistent storage\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1633723469845_1020105713",
      "id": "paragraph_1633723469845_1020105713",
      "dateCreated": "2021-10-08 17:04:29.845",
      "dateStarted": "2021-10-08 17:04:59.884",
      "dateFinished": "2021-10-08 17:04:59.903",
      "status": "FINISHED"
    },
    {
      "text": "%md\n\n   * PersistentVolumes.yaml\n```yaml\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: volume-park-worker\nspec:\n  storageClassName: pure-file\n  accessModes:\n    - ReadWriteMany\n  capacity:\n    storage: 30Gi\n  hostPath:\n    path: /lab/data\n```\n   \n```commandline\n   kubectl apply -f PersistentVolumes.yaml\n```   ",
      "user": "anonymous",
      "dateUpdated": "2021-10-08 17:32:23.638",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cul\u003e\n\u003cli\u003ePersistentVolumes.yaml\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class\u003d\"language-yaml\"\u003eapiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: volume-park-worker\nspec:\n  storageClassName: pure-file\n  accessModes:\n    - ReadWriteMany\n  capacity:\n    storage: 30Gi\n  hostPath:\n    path: /lab/data\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class\u003d\"language-commandline\"\u003e   kubectl apply -f PersistentVolumes.yaml\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1633723237186_665535523",
      "id": "paragraph_1633723237186_665535523",
      "dateCreated": "2021-10-08 17:00:37.186",
      "dateStarted": "2021-10-08 17:32:23.638",
      "dateFinished": "2021-10-08 17:32:23.650",
      "status": "FINISHED"
    },
    {
      "text": "%md\n   * PersistenVolumeClaim.yaml\n```yaml\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: share\nspec:\n  storageClassName: pure-file\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 5Gi\n```      \n   \n```commandline   \n    kubectl appy -f PersistenVolumeClaim.yaml\n```\n",
      "user": "anonymous",
      "dateUpdated": "2021-10-11 17:36:12.680",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cul\u003e\n\u003cli\u003ePersistenVolumeClaim.yaml\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class\u003d\"language-yaml\"\u003ekind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: share\nspec:\n  storageClassName: pure-file\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 5Gi\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class\u003d\"language-commandline\"\u003e    kubectl appy -f PersistenVolumeClaim.yaml\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1633723141851_1747434200",
      "id": "paragraph_1633723141851_1747434200",
      "dateCreated": "2021-10-08 16:59:01.851",
      "dateStarted": "2021-10-11 17:36:12.679",
      "dateFinished": "2021-10-11 17:36:18.036",
      "status": "FINISHED"
    },
    {
      "text": "%md\n\n```commandline\nminikube ssh mkdir -p /lab/data/sh\n```\n",
      "user": "anonymous",
      "dateUpdated": "2021-10-08 17:30:51.827",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cpre\u003e\u003ccode class\u003d\"language-commandline\"\u003eminikube ssh mkdir -p /lab/data/sh\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1633724999001_330181226",
      "id": "paragraph_1633724999001_330181226",
      "dateCreated": "2021-10-08 17:29:59.002",
      "dateStarted": "2021-10-08 17:30:51.827",
      "dateFinished": "2021-10-08 17:30:51.847",
      "status": "FINISHED"
    },
    {
      "text": "%md\n# SPARK shell on Kubernetes with Delta\n```\n$SPARK_HOME/bin/spark-shell --verbose --deploy-mode client --master k8s://https://192.168.176.7:8443 \\\n                            --name spark-sample \\\n                            --num-executors 2 \\\n                            --executor-memory 4g \\\n                            --packages io.delta:delta-core_2.12:1.0.0,org.apache.hadoop:hadoop-aws:$HADOOP_VERSION,org.apache.hadoop:hadoop-client:$HADOOP_VERSION,org.apache.hadoop:hadoop-common:$HADOOP_VERSION \\\n                            --conf \"spark.kubernetes.namespace\u003dzeppelin\" \\\n                            --conf \"spark.sql.extensions\u003dio.delta.sql.DeltaSparkSessionExtension\"  \\\n                            --conf \"spark.sql.catalog.spark_catalog\u003dorg.apache.spark.sql.delta.catalog.DeltaCatalog\" \\\n                            --conf \"spark.kubernetes.container.image\u003drogermm/spark-base-python:master\" \\\n                            \\\n                           --conf spark.kubernetes.driver.volumes.persistentVolumeClaim.share.options.claimName\u003dhost-claim \\\n                           --conf spark.kubernetes.driver.volumes.persistentVolumeClaim.share.mount.path\u003d/share \\\n                           --conf spark.kubernetes.driver.volumes.persistentVolumeClaim.share.mount.subPath\u003dshare \\\n                           --conf spark.kubernetes.executor.volumes.persistentVolumeClaim.share.options.claimName\u003dhost-claim \\\n                           --conf spark.kubernetes.executor.volumes.persistentVolumeClaim.share.mount.path\u003d/share \\\n                           --conf spark.kubernetes.executor.volumes.persistentVolumeClaim.share.mount.subPath\u003d\n```                       ",
      "user": "anonymous",
      "dateUpdated": "2021-10-13 11:42:31.251",
      "progress": 0,
      "config": {
        "lineNumbers": false,
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "title": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch1\u003eSPARK shell on Kubernetes with Delta\u003c/h1\u003e\n\u003cpre\u003e\u003ccode\u003e$SPARK_HOME/bin/spark-shell --verbose --deploy-mode client --master k8s://https://192.168.176.7:8443 \\\n                            --name spark-sample \\\n                            --num-executors 2 \\\n                            --executor-memory 4g \\\n                            --packages io.delta:delta-core_2.12:1.0.0,org.apache.hadoop:hadoop-aws:$HADOOP_VERSION,org.apache.hadoop:hadoop-client:$HADOOP_VERSION,org.apache.hadoop:hadoop-common:$HADOOP_VERSION \\\n                            --conf \u0026quot;spark.kubernetes.namespace\u003dzeppelin\u0026quot; \\\n                            --conf \u0026quot;spark.sql.extensions\u003dio.delta.sql.DeltaSparkSessionExtension\u0026quot;  \\\n                            --conf \u0026quot;spark.sql.catalog.spark_catalog\u003dorg.apache.spark.sql.delta.catalog.DeltaCatalog\u0026quot; \\\n                            --conf \u0026quot;spark.kubernetes.container.image\u003drogermm/spark-base-python:master\u0026quot; \\\n                            \\\n                           --conf spark.kubernetes.driver.volumes.persistentVolumeClaim.share.options.claimName\u003dhost-claim \\\n                           --conf spark.kubernetes.driver.volumes.persistentVolumeClaim.share.mount.path\u003d/share \\\n                           --conf spark.kubernetes.driver.volumes.persistentVolumeClaim.share.mount.subPath\u003dshare \\\n                           --conf spark.kubernetes.executor.volumes.persistentVolumeClaim.share.options.claimName\u003dhost-claim \\\n                           --conf spark.kubernetes.executor.volumes.persistentVolumeClaim.share.mount.path\u003d/share \\\n                           --conf spark.kubernetes.executor.volumes.persistentVolumeClaim.share.mount.subPath\u003d\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1633720105763_694442924",
      "id": "paragraph_1633720105763_694442924",
      "dateCreated": "2021-10-08 16:08:25.763",
      "dateStarted": "2021-10-13 11:42:31.251",
      "dateFinished": "2021-10-13 11:42:31.293",
      "status": "FINISHED"
    },
    {
      "text": "%md\n# SPARK submit example\n```\n$SPARK_HOME/bin/spark-submit --name sparkpi-test1  \\\n  --master  k8s://https://192.168.176.5:8443 \\\n  --deploy-mode cluster \\\n  --class org.apache.spark.examples.SparkPi \\\n  --conf spark.kubernetes.authenticate.driver.serviceAccountName\u003dspark \\\n  --conf spark.kubernetes.namespace\u003dzeppelin \\\n  --conf spark.executor.instances\u003d2 \\\n  --conf spark.kubernetes.container.image\u003djboothomas/spark-py:k8sv3.0.0 \\\n  --conf spark.kubernetes.container.image.pullPolicy\u003dAlways \\\n  --conf spark.eventLog.enabled\u003dtrue \\\n  --conf spark.eventLog.dir\u003d/opt/spark/work-dir \\\n  \\\n  --conf \"spark.kubernetes.driver.volumes.persistentVolumeClaim.share.options.claimName\u003dhost-claim\" \\\n  --conf \"spark.kubernetes.driver.volumes.persistentVolumeClaim.share.mount.path\u003d/opt/spark/work-dir\" \\\n  --conf \"spark.kubernetes.executor.volumes.persistentVolumeClaim.share.options.claimName\u003dhost-claim\" \\\n  --conf \"spark.kubernetes.executor.volumes.persistentVolumeClaim.share.mount.path\u003d/opt/spark/work-dir\" \\\n  \\\n  local://$SPARK_HOME/examples/jars/spark-examples_2.12-3.1.2.jar\n```   ",
      "user": "anonymous",
      "dateUpdated": "2021-10-13 11:19:37.963",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch1\u003eSPARK submit example\u003c/h1\u003e\n\u003cpre\u003e\u003ccode\u003e$SPARK_HOME/bin/spark-submit --name sparkpi-test1  \\\n  --master  k8s://https://192.168.176.5:8443 \\\n  --deploy-mode cluster \\\n  --class org.apache.spark.examples.SparkPi \\\n  --conf spark.kubernetes.authenticate.driver.serviceAccountName\u003dspark \\\n  --conf spark.kubernetes.namespace\u003dzeppelin \\\n  --conf spark.executor.instances\u003d2 \\\n  --conf spark.kubernetes.container.image\u003djboothomas/spark-py:k8sv3.0.0 \\\n  --conf spark.kubernetes.container.image.pullPolicy\u003dAlways \\\n  --conf spark.eventLog.enabled\u003dtrue \\\n  --conf spark.eventLog.dir\u003d/opt/spark/work-dir \\\n  \\\n  --conf \u0026quot;spark.kubernetes.driver.volumes.persistentVolumeClaim.share.options.claimName\u003dhost-claim\u0026quot; \\\n  --conf \u0026quot;spark.kubernetes.driver.volumes.persistentVolumeClaim.share.mount.path\u003d/opt/spark/work-dir\u0026quot; \\\n  --conf \u0026quot;spark.kubernetes.executor.volumes.persistentVolumeClaim.share.options.claimName\u003dhost-claim\u0026quot; \\\n  --conf \u0026quot;spark.kubernetes.executor.volumes.persistentVolumeClaim.share.mount.path\u003d/opt/spark/work-dir\u0026quot; \\\n  \\\n  local://$SPARK_HOME/examples/jars/spark-examples_2.12-3.1.2.jar\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1633719630530_678121597",
      "id": "paragraph_1633719630530_678121597",
      "dateCreated": "2021-10-08 16:00:30.530",
      "dateStarted": "2021-10-13 11:19:37.963",
      "dateFinished": "2021-10-13 11:19:37.996",
      "status": "FINISHED"
    },
    {
      "text": "%md\n\n# Driver\n\n```\n/usr/local/openjdk-11/bin/java \n\n-cp     /tmp/local-repo/spark-delta/*:\n        /opt/zeppelin/interpreter/spark/*:\n        /opt/zeppelin/interpreter/zeppelin-interpreter-shaded-0.10.0.jar:\n        /opt/zeppelin/interpreter/spark/spark-interpreter-0.10.0.jar:\n        /opt/spark/conf:/opt/spark/jars/*:\n        /opt/hadoop-3.3.1/etc/hadoop/ \n\n-Xmx8g \n-Dfile.encoding\u003dUTF-8 \n-Dlog4j.configuration\u003dfile:///opt/zeppelin/conf/log4j.properties \n-Dlog4j.configurationFile\u003dfile:///opt/zeppelin/conf/log4j2.properties -Dzeppelin.log.file\u003d/opt/zeppelin/logs/zeppelin-interpreter-spark-delta-shared_process--spark-qifkbg.log org.apache.spark.deploy.SparkSubmit \n--master k8s://https://kubernetes.default.svc\n--deploy-mode client\n\n--conf spark.driver.memory\u003d8g\n--conf spark.kubernetes.namespace\u003dzeppelin\n--conf spark.driver.port\u003d22321\n--conf spark.kubernetes.container.image\u003drogermm/spark-base-python:master \n--conf spark.driver.extraClassPath\u003d    :\n                                       /tmp/local-repo/spark-delta/*:\n                                       /opt/zeppelin/interpreter/spark/*:\n                                       :\n                                       :\n                                       /opt/zeppelin/interpreter/zeppelin-interpreter-shaded-0.10.0.jar:\n                                       /opt/zeppelin/interpreter/spark/spark-interpreter-0.10.0.jar\n\n--conf spark.kubernetes.driver.pod.name\u003dspark-qifkbg \n--conf spark.driver.bindAddress\u003d0.0.0.0 \n--conf spark.executor.instances\u003d1 \n--conf spark.driver.host\u003dspark-qifkbg.zeppelin.svc\n--conf spark.blockManager.port\u003d22322\n--conf spark.driver.extraJavaOptions\u003d\n\n-Dfile.encoding\u003dUTF-8 \n-Dlog4j.configuration\u003dfile:///opt/zeppelin/conf/log4j.properties -Dlog4j.configurationFile\u003dfile:///opt/zeppelin/conf/log4j2.properties -Dzeppelin.log.file\u003d/opt/zeppelin/logs/zeppelin-interpreter-spark-delta-shared_process--spark-qifkbg.log\n\n--class org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer\n\n/opt/zeppelin/interpreter/spark/spark-interpreter-0.10.0.jar \n\nzeppelin-server.zeppelin.svc 12320 spark-delta-shared_process 12321:12321\n```",
      "user": "anonymous",
      "dateUpdated": "2021-10-13 15:10:13.503",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch1\u003eDriver\u003c/h1\u003e\n\u003cpre\u003e\u003ccode\u003e/usr/local/openjdk-11/bin/java \n\n-cp     /tmp/local-repo/spark-delta/*:\n        /opt/zeppelin/interpreter/spark/*:\n        /opt/zeppelin/interpreter/zeppelin-interpreter-shaded-0.10.0.jar:\n        /opt/zeppelin/interpreter/spark/spark-interpreter-0.10.0.jar:\n        /opt/spark/conf:/opt/spark/jars/*:\n        /opt/hadoop-3.3.1/etc/hadoop/ \n\n-Xmx8g \n-Dfile.encoding\u003dUTF-8 \n-Dlog4j.configuration\u003dfile:///opt/zeppelin/conf/log4j.properties \n-Dlog4j.configurationFile\u003dfile:///opt/zeppelin/conf/log4j2.properties -Dzeppelin.log.file\u003d/opt/zeppelin/logs/zeppelin-interpreter-spark-delta-shared_process--spark-qifkbg.log org.apache.spark.deploy.SparkSubmit \n--master k8s://https://kubernetes.default.svc\n--deploy-mode client\n\n--conf spark.driver.memory\u003d8g\n--conf spark.kubernetes.namespace\u003dzeppelin\n--conf spark.driver.port\u003d22321\n--conf spark.kubernetes.container.image\u003drogermm/spark-base-python:master \n--conf spark.driver.extraClassPath\u003d    :\n                                       /tmp/local-repo/spark-delta/*:\n                                       /opt/zeppelin/interpreter/spark/*:\n                                       :\n                                       :\n                                       /opt/zeppelin/interpreter/zeppelin-interpreter-shaded-0.10.0.jar:\n                                       /opt/zeppelin/interpreter/spark/spark-interpreter-0.10.0.jar\n\n--conf spark.kubernetes.driver.pod.name\u003dspark-qifkbg \n--conf spark.driver.bindAddress\u003d0.0.0.0 \n--conf spark.executor.instances\u003d1 \n--conf spark.driver.host\u003dspark-qifkbg.zeppelin.svc\n--conf spark.blockManager.port\u003d22322\n--conf spark.driver.extraJavaOptions\u003d\n\n-Dfile.encoding\u003dUTF-8 \n-Dlog4j.configuration\u003dfile:///opt/zeppelin/conf/log4j.properties -Dlog4j.configurationFile\u003dfile:///opt/zeppelin/conf/log4j2.properties -Dzeppelin.log.file\u003d/opt/zeppelin/logs/zeppelin-interpreter-spark-delta-shared_process--spark-qifkbg.log\n\n--class org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer\n\n/opt/zeppelin/interpreter/spark/spark-interpreter-0.10.0.jar \n\nzeppelin-server.zeppelin.svc 12320 spark-delta-shared_process 12321:12321\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1634146710036_543945919",
      "id": "paragraph_1634146710036_543945919",
      "dateCreated": "2021-10-13 14:38:30.036",
      "dateStarted": "2021-10-13 15:10:13.503",
      "dateFinished": "2021-10-13 15:10:13.548",
      "status": "FINISHED"
    },
    {
      "text": "%md\n\n# Remote interpreter\n\n```\n/usr/bin/tini -s -- \n```\n\n\n```\n/usr/local/openjdk-11/bin/java\n\n-Dspark.blockManager.port\u003d22322\n-Dspark.driver.port\u003d22321 \n\n-Xms8192m \n-Xmx8192m\n\n-cp /opt/spark/conf:\n    :\n    /opt/spark/jars/*:\n    /opt/hadoop-3.3.1/etc/hadoop:\n    /opt/hadoop-3.3.1/share/hadoop/common/lib/*:\n    /opt/hadoop-3.3.1/share/hadoop/common/*:\n    /opt/hadoop-3.3.1/share/hadoop/hdfs:\n    /opt/hadoop-3.3.1/share/hadoop/hdfs/lib/*\n    :/opt/hadoop-3.3.1/share/hadoop/hdfs/*:\n    /opt/hadoop-3.3.1/share/hadoop/mapreduce/*:\n    /opt/hadoop-3.3.1/share/hadoop/yarn:\n    /opt/hadoop-3.3.1/share/hadoop/yarn/lib/*:\n    /opt/hadoop-3.3.1/share/hadoop/yarn/*\n    \n   org.apache.spark.executor.CoarseGrainedExecutorBackend\n\n   --driver-url spark://CoarseGrainedScheduler@spark-qifkbg.zeppelin.svc:22321\n   --executor-id 1\n   --cores 4\n   --app-id spark-application-1634146385747\n   --hostname 172.17.0.10\n   --resourceProfileId 0\n```\n\n",
      "user": "anonymous",
      "dateUpdated": "2021-10-13 15:16:26.439",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch1\u003eRemote interpreter\u003c/h1\u003e\n\u003cpre\u003e\u003ccode\u003e/usr/bin/tini -s -- \n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003e/usr/local/openjdk-11/bin/java\n\n-Dspark.blockManager.port\u003d22322\n-Dspark.driver.port\u003d22321 \n\n-Xms8192m \n-Xmx8192m\n\n-cp /opt/spark/conf:\n    :\n    /opt/spark/jars/*:\n    /opt/hadoop-3.3.1/etc/hadoop:\n    /opt/hadoop-3.3.1/share/hadoop/common/lib/*:\n    /opt/hadoop-3.3.1/share/hadoop/common/*:\n    /opt/hadoop-3.3.1/share/hadoop/hdfs:\n    /opt/hadoop-3.3.1/share/hadoop/hdfs/lib/*\n    :/opt/hadoop-3.3.1/share/hadoop/hdfs/*:\n    /opt/hadoop-3.3.1/share/hadoop/mapreduce/*:\n    /opt/hadoop-3.3.1/share/hadoop/yarn:\n    /opt/hadoop-3.3.1/share/hadoop/yarn/lib/*:\n    /opt/hadoop-3.3.1/share/hadoop/yarn/*\n    \n   org.apache.spark.executor.CoarseGrainedExecutorBackend\n\n   --driver-url spark://CoarseGrainedScheduler@spark-qifkbg.zeppelin.svc:22321\n   --executor-id 1\n   --cores 4\n   --app-id spark-application-1634146385747\n   --hostname 172.17.0.10\n   --resourceProfileId 0\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1634148476804_1035479665",
      "id": "paragraph_1634148476804_1035479665",
      "dateCreated": "2021-10-13 15:07:56.804",
      "dateStarted": "2021-10-13 15:16:26.439",
      "dateFinished": "2021-10-13 15:16:26.474",
      "status": "FINISHED"
    },
    {
      "text": "%sh\nls /share\nrm -rf /share/delta/delta-table\nls /share",
      "user": "anonymous",
      "dateUpdated": "2021-10-14 10:06:25.852",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sh",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "delta\ndelta\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1632248651658_59745080",
      "id": "paragraph_1631727335161_1183080461",
      "dateCreated": "2021-09-21 15:24:11.658",
      "dateStarted": "2021-10-14 10:06:25.927",
      "dateFinished": "2021-10-14 10:06:33.521",
      "status": "FINISHED"
    },
    {
      "text": "%spark-delta\n\nprint(\"Show configuration\\n\")\n\nval arrayConfig\u003dspark.sparkContext.getConf.getAll\n\nfor (conf \u003c- arrayConfig)\n    println(\"--\u003e\" + conf._1 +\"                              , \"+ conf._2)\n",
      "user": "anonymous",
      "dateUpdated": "2021-10-14 10:06:49.331",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Show configuration\n--\u003espark.driver.extraClassPath                              , :/tmp/local-repo/spark-delta/*:/opt/zeppelin/interpreter/spark/*:::/opt/zeppelin/interpreter/zeppelin-interpreter-shaded-0.10.0.jar:/opt/zeppelin/interpreter/spark/spark-interpreter-0.10.0.jar\n--\u003espark.driver.cores                              , 4\n--\u003espark.app.name                              , org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer\n--\u003ezeppelin.pyspark.useIPython                              , true\n--\u003espark.app.startTime                              , 1634216823902\n--\u003espark.jars.packages                              , io.delta:delta-core_2.12:1.0.0,org.apache.hadoop:hadoop-aws:3.3.1,org.apache.hadoop:hadoop-client:3.3.1,org.apache.hadoop:hadoop-common:3.3.1\n--\u003ezeppelin.spark.concurrentSQL                              , true\n--\u003espark.kubernetes.container.image                              , rogermm/spark-base-python:master\n--\u003espark.driver.extraJavaOptions                              ,  -Dfile.encoding\u003dUTF-8 -Dlog4j.configuration\u003dfile:///opt/zeppelin/conf/log4j.properties -Dlog4j.configurationFile\u003dfile:///opt/zeppelin/conf/log4j2.properties -Dzeppelin.log.file\u003d/opt/zeppelin/logs/zeppelin-interpreter-spark-delta-shared_process--spark-oxvual.log\n--\u003ezeppelin.spark.run.asLoginUser                              , true\n--\u003ezeppelin.interpreter.connection.poolsize                              , 100\n--\u003espark.webui.yarn.useProxy                              , false\n--\u003espark.useHiveContext                              , true\n--\u003espark.app.id                              , spark-application-1634216827112\n--\u003eSPARK_HOME                              , /opt/spark\n--\u003ezeppelin.kotlin.shortenTypes                              , true\n--\u003ezeppelin.R.shiny.portRange                              , :\n--\u003ezeppelin.spark.printREPLOutput                              , true\n--\u003ezeppelin.spark.enableSupportedVersionCheck                              , true\n--\u003ezeppelin.spark.maxResult                              , 1000\n--\u003espark.executor.id                              , driver\n--\u003ezeppelin.R.image.width                              , 100%\n--\u003ezeppelin.spark.ui.hidden                              , false\n--\u003ezeppelin.spark.deprecatedMsg.show                              , true\n--\u003ezeppelin.spark.sql.interpolation                              , false\n--\u003espark.kubernetes.executor.volumes.persistentVolumeClaim.share.options.claimName                              , host-claim\n--\u003espark.repl.class.uri                              , spark://spark-oxvual.zeppelin.svc:22321/classes\n--\u003espark.master                              , k8s://https://kubernetes.default.svc\n--\u003espark.kubernetes.executor.volumes.persistentVolumeClaim.share.mount.path                              , /share\n--\u003ezeppelin.interpreter.localRepo                              , /opt/zeppelin/local-repo/spark-delta\n--\u003ezeppelin.R.cmd                              , R\n--\u003espark.kubernetes.driver.pod.name                              , spark-oxvual\n--\u003espark.executor.cores                              , 4\n--\u003ezeppelin.interpreter.output.limit                              , 102400\n--\u003ePYSPARK_PYTHON                              , python\n--\u003espark.executor.instances                              , 2\n--\u003ePYSPARK_DRIVER_PYTHON                              , python\n--\u003espark.driver.memory                              , 8g\n--\u003espark.kubernetes.executor.podNamePrefix                              , org-apache-zeppelin-interpreter-remote-remoteinterpreterserver-a4a21e7c7ee9d28c\n--\u003espark.executor.memory                              , 8g\n--\u003espark.driver.host                              , spark-oxvual.zeppelin.svc\n--\u003ezeppelin.R.knitr                              , true\n--\u003espark.submit.deployMode                              , client\n--\u003espark.jars                              , /root/.ivy2/jars/io.delta_delta-core_2.12-1.0.0.jar\n--\u003ezeppelin.spark.useHiveContext                              , true\n--\u003espark.scheduler.mode                              , FAIR\n--\u003ezeppelin.spark.uiWebUrl                              , //4040-spark-oxvual.local.zeppelin-project.org:8080\n--\u003ezeppelin.R.render.options                              , out.format \u003d \u0027html\u0027, comment \u003d NA, echo \u003d FALSE, results \u003d \u0027asis\u0027, message \u003d F, warning \u003d F, fig.retina \u003d 2\n--\u003espark.blockManager.port                              , 22322\n--\u003espark.kubernetes.namespace                              , zeppelin\n--\u003espark.driver.bindAddress                              , 0.0.0.0\n--\u003espark.sql.extensions                              , io.delta.sql.DeltaSparkSessionExtension\n--\u003espark.repl.class.outputDir                              , /tmp/spark8918777826530324370\n--\u003espark.driver.port                              , 22321\n--\u003espark.submit.pyFiles                              , \n--\u003ezeppelin.spark.concurrentSQL.max                              , 10\n--\u003espark.app.initial.jar.urls                              , spark://spark-oxvual.zeppelin.svc:22321/jars/io.delta_delta-core_2.12-1.0.0.jar\n--\u003ezeppelin.spark.sql.stacktrace                              , true\n--\u003ezeppelin.spark.scala.color                              , true\n--\u003espark.sql.catalog.spark_catalog                              , org.apache.spark.sql.delta.catalog.DeltaCatalog\n\u001b[1m\u001b[34marrayConfig\u001b[0m: \u001b[1m\u001b[32mArray[(String, String)]\u001b[0m \u003d Array((spark.driver.extraClassPath,:/tmp/local-repo/spark-delta/*:/opt/zeppelin/interpreter/spark/*:::/opt/zeppelin/interpreter/zeppelin-interpreter-shaded-0.10.0.jar:/opt/zeppelin/interpreter/spark/spark-interpreter-0.10.0.jar), (spark.driver.cores,4), (spark.app.name,org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer), (zeppelin.pyspark.useIPython,true), (spark.app.startTime,1634216823902), (spark.jars.packages,io.delta:delta-core_2.12:1.0.0,org.apache.hadoop:hadoop-aws:3.3.1,org.apache.hadoop:hadoop-client:3.3.1,org.apache.hadoop:hadoop-common:3.3.1), (zeppelin.spark.concurrentSQL,true), (spark.kubernetes.container.image,rogermm/spark-base-python:master), (spark.driver.extraJavaOptions,\" -Dfile.encoding\u003dUTF-...\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1634144102718_946030607",
      "id": "paragraph_1634144102718_946030607",
      "dateCreated": "2021-10-13 13:55:02.718",
      "dateStarted": "2021-10-14 10:06:49.392",
      "dateFinished": "2021-10-14 10:07:18.948",
      "status": "FINISHED"
    },
    {
      "title": "Create a table",
      "text": "%spark-delta\nvar share\u003d\"/share/delta/\"\nvar table\u003dshare+\"delta-table\"\n\nval data \u003d spark.range(0, 5)\ndata.write.format(\"delta\").save(table)\n",
      "user": "anonymous",
      "dateUpdated": "2021-10-14 10:07:26.527",
      "progress": 100,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 9.0,
        "title": true,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 228.0,
              "optionOpen": false
            }
          }
        },
        "enabled": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mshare\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m \u003d /share/delta/\n\u001b[1m\u001b[34mtable\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m \u003d /share/delta/delta-table\n\u001b[1m\u001b[34mdata\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[Long]\u001b[0m \u003d [id: bigint]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "//4040-spark-oxvual.local.zeppelin-project.org:8080/jobs/job?id\u003d0"
            },
            {
              "jobUrl": "//4040-spark-oxvual.local.zeppelin-project.org:8080/jobs/job?id\u003d1"
            }
          ],
          "interpreterSettingId": "spark-delta"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1632248651658_433128204",
      "id": "paragraph_1588147833426_1914590471",
      "dateCreated": "2021-09-21 15:24:11.659",
      "dateStarted": "2021-10-14 10:07:26.585",
      "dateFinished": "2021-10-14 10:07:48.294",
      "status": "FINISHED"
    },
    {
      "title": "Read a table",
      "text": "%spark-delta\n\nval df \u003d spark.read.format(\"delta\").load(table)\ndf.show()\n",
      "user": "anonymous",
      "dateUpdated": "2021-10-14 10:08:36.565",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 6.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+---+\n| id|\n+---+\n|  3|\n|  1|\n|  2|\n|  0|\n|  4|\n+---+\n\n\u001b[1m\u001b[34mdf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m \u003d [id: bigint]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "//4040-spark-oxvual.local.zeppelin-project.org:8080/jobs/job?id\u003d2"
            },
            {
              "jobUrl": "//4040-spark-oxvual.local.zeppelin-project.org:8080/jobs/job?id\u003d3"
            },
            {
              "jobUrl": "//4040-spark-oxvual.local.zeppelin-project.org:8080/jobs/job?id\u003d4"
            },
            {
              "jobUrl": "//4040-spark-oxvual.local.zeppelin-project.org:8080/jobs/job?id\u003d5"
            }
          ],
          "interpreterSettingId": "spark-delta"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1632248651659_1856748697",
      "id": "paragraph_1588147853461_1624743216",
      "dateCreated": "2021-09-21 15:24:11.659",
      "dateStarted": "2021-10-14 10:08:36.628",
      "dateFinished": "2021-10-14 10:08:39.226",
      "status": "FINISHED"
    },
    {
      "title": "Overwrite",
      "text": "%spark-delta\n\nval data \u003d spark.range(5, 10)\ndata.write.format(\"delta\").mode(\"overwrite\").save(table)\ndf.show()\n",
      "user": "anonymous",
      "dateUpdated": "2021-10-14 10:17:51.194",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 6.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+---+\n| id|\n+---+\n|  6|\n|  9|\n|  5|\n|  8|\n|  7|\n+---+\n\n\u001b[1m\u001b[34mdata\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[Long]\u001b[0m \u003d [id: bigint]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "//4040-spark-oxvual.local.zeppelin-project.org:8080/jobs/job?id\u003d6"
            },
            {
              "jobUrl": "//4040-spark-oxvual.local.zeppelin-project.org:8080/jobs/job?id\u003d7"
            },
            {
              "jobUrl": "//4040-spark-oxvual.local.zeppelin-project.org:8080/jobs/job?id\u003d8"
            },
            {
              "jobUrl": "//4040-spark-oxvual.local.zeppelin-project.org:8080/jobs/job?id\u003d9"
            },
            {
              "jobUrl": "//4040-spark-oxvual.local.zeppelin-project.org:8080/jobs/job?id\u003d10"
            },
            {
              "jobUrl": "//4040-spark-oxvual.local.zeppelin-project.org:8080/jobs/job?id\u003d11"
            },
            {
              "jobUrl": "//4040-spark-oxvual.local.zeppelin-project.org:8080/jobs/job?id\u003d12"
            }
          ],
          "interpreterSettingId": "spark-delta"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1632248651660_1271754293",
      "id": "paragraph_1588148062120_1790808564",
      "dateCreated": "2021-09-21 15:24:11.660",
      "dateStarted": "2021-10-14 10:17:51.251",
      "dateFinished": "2021-10-14 10:17:56.476",
      "status": "FINISHED"
    },
    {
      "title": "Conditional update without overwrite",
      "text": "%spark-delta\n\nimport io.delta.tables._\nimport org.apache.spark.sql.functions._\n\nval deltaTable \u003d DeltaTable.forPath(table)\n\n// Update every even value by adding 100 to it\ndeltaTable.update(\n  condition \u003d expr(\"id % 2 \u003d\u003d 0\"),\n  set \u003d Map(\"id\" -\u003e expr(\"id + 100\")))\n\n// Delete every even value\ndeltaTable.delete(condition \u003d expr(\"id % 2 \u003d\u003d 0\"))\n\n// Upsert (merge) new data\nval newData \u003d spark.range(0, 20).toDF\n\ndeltaTable.as(\"oldData\")\n  .merge(\n    newData.as(\"newData\"),\n    \"oldData.id \u003d newData.id\")\n  .whenMatched\n  .update(Map(\"id\" -\u003e col(\"newData.id\")))\n  .whenNotMatched\n  .insert(Map(\"id\" -\u003e col(\"newData.id\")))\n  .execute()\n\ndeltaTable.toDF.show()",
      "user": "anonymous",
      "dateUpdated": "2021-10-14 10:18:04.282",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 6.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+---+\n| id|\n+---+\n| 17|\n|  7|\n|  6|\n| 15|\n| 16|\n| 19|\n|  3|\n|  4|\n|  1|\n|  8|\n| 14|\n|  5|\n|  9|\n| 12|\n| 18|\n| 11|\n|  0|\n| 10|\n|  2|\n| 13|\n+---+\n\nimport io.delta.tables._\nimport org.apache.spark.sql.functions._\n\u001b[1m\u001b[34mdeltaTable\u001b[0m: \u001b[1m\u001b[32mio.delta.tables.DeltaTable\u001b[0m \u003d io.delta.tables.DeltaTable@3364dad0\n\u001b[1m\u001b[34mnewData\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m \u003d [id: bigint]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "//4040-spark-oxvual.local.zeppelin-project.org:8080/jobs/job?id\u003d18"
            },
            {
              "jobUrl": "//4040-spark-oxvual.local.zeppelin-project.org:8080/jobs/job?id\u003d19"
            },
            {
              "jobUrl": "//4040-spark-oxvual.local.zeppelin-project.org:8080/jobs/job?id\u003d20"
            },
            {
              "jobUrl": "//4040-spark-oxvual.local.zeppelin-project.org:8080/jobs/job?id\u003d21"
            },
            {
              "jobUrl": "//4040-spark-oxvual.local.zeppelin-project.org:8080/jobs/job?id\u003d22"
            },
            {
              "jobUrl": "//4040-spark-oxvual.local.zeppelin-project.org:8080/jobs/job?id\u003d23"
            },
            {
              "jobUrl": "//4040-spark-oxvual.local.zeppelin-project.org:8080/jobs/job?id\u003d24"
            },
            {
              "jobUrl": "//4040-spark-oxvual.local.zeppelin-project.org:8080/jobs/job?id\u003d25"
            },
            {
              "jobUrl": "//4040-spark-oxvual.local.zeppelin-project.org:8080/jobs/job?id\u003d26"
            },
            {
              "jobUrl": "//4040-spark-oxvual.local.zeppelin-project.org:8080/jobs/job?id\u003d28"
            },
            {
              "jobUrl": "//4040-spark-oxvual.local.zeppelin-project.org:8080/jobs/job?id\u003d29"
            },
            {
              "jobUrl": "//4040-spark-oxvual.local.zeppelin-project.org:8080/jobs/job?id\u003d30"
            },
            {
              "jobUrl": "//4040-spark-oxvual.local.zeppelin-project.org:8080/jobs/job?id\u003d31"
            },
            {
              "jobUrl": "//4040-spark-oxvual.local.zeppelin-project.org:8080/jobs/job?id\u003d32"
            },
            {
              "jobUrl": "//4040-spark-oxvual.local.zeppelin-project.org:8080/jobs/job?id\u003d33"
            },
            {
              "jobUrl": "//4040-spark-oxvual.local.zeppelin-project.org:8080/jobs/job?id\u003d34"
            }
          ],
          "interpreterSettingId": "spark-delta"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1632248651660_2034341072",
      "id": "paragraph_1588147954117_626957150",
      "dateCreated": "2021-09-21 15:24:11.661",
      "dateStarted": "2021-10-14 10:18:04.340",
      "dateFinished": "2021-10-14 10:18:19.995",
      "status": "FINISHED"
    },
    {
      "title": "Read older versions of data using time travel",
      "text": "%spark-delta\n\nval df \u003d spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(table)\ndf.show()",
      "user": "anonymous",
      "dateUpdated": "2021-10-14 10:17:58.428",
      "progress": 100,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 6.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+---+\n| id|\n+---+\n|  3|\n|  1|\n|  2|\n|  0|\n|  4|\n+---+\n\n\u001b[1m\u001b[34mdf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m \u003d [id: bigint]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "//4040-spark-oxvual.local.zeppelin-project.org:8080/jobs/job?id\u003d13"
            },
            {
              "jobUrl": "//4040-spark-oxvual.local.zeppelin-project.org:8080/jobs/job?id\u003d14"
            },
            {
              "jobUrl": "//4040-spark-oxvual.local.zeppelin-project.org:8080/jobs/job?id\u003d15"
            },
            {
              "jobUrl": "//4040-spark-oxvual.local.zeppelin-project.org:8080/jobs/job?id\u003d16"
            },
            {
              "jobUrl": "//4040-spark-oxvual.local.zeppelin-project.org:8080/jobs/job?id\u003d17"
            }
          ],
          "interpreterSettingId": "spark-delta"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1632248651661_1024723993",
      "id": "paragraph_1588148133131_1770029903",
      "dateCreated": "2021-09-21 15:24:11.661",
      "dateStarted": "2021-10-14 10:17:58.475",
      "dateFinished": "2021-10-14 10:18:00.933",
      "status": "FINISHED"
    },
    {
      "text": "%spark-delta\n",
      "user": "anonymous",
      "dateUpdated": "2021-09-21 15:24:11.662",
      "progress": 0,
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1632248651662_1255104187",
      "id": "paragraph_1631307360624_1840309077",
      "dateCreated": "2021-09-21 15:24:11.662",
      "status": "READY"
    }
  ],
  "name": "tutorial-1",
  "id": "2GJ5BXZY7",
  "defaultInterpreterGroup": "spark-delta",
  "version": "0.10.0",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}